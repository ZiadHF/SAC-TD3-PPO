{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# MLP Networks\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation=nn.ReLU, output_activation=None):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.extend([nn.Linear(dims[i], dims[i+1]), activation()])\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "        if output_activation:\n",
    "            layers.append(output_activation())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dims=[256, 256], log_std_min=-20, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.net = MLP(obs_dim, hidden_dims, action_dim * 2)\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, obs):\n",
    "        output = self.net(obs)\n",
    "        mean, log_std = output.chunk(2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, obs, deterministic=False):\n",
    "        mean, log_std = self.forward(obs)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        if deterministic:\n",
    "            return torch.tanh(mean), None\n",
    "        \n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.net = MLP(obs_dim + action_dim, hidden_dims, 1)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        return self.net(torch.cat([obs, action], dim=-1))\n",
    "\n",
    "class TwinCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.q1 = Critic(obs_dim, action_dim, hidden_dims)\n",
    "        self.q2 = Critic(obs_dim, action_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        return self.q1(obs, action), self.q2(obs, action)\n",
    "\n",
    "    def min_q(self, obs, action):\n",
    "        q1, q2 = self.forward(obs, action)\n",
    "        return torch.min(q1, q2)\n",
    "\n",
    "# CNN Networks\n",
    "class NatureCNN(nn.Module):\n",
    "    def __init__(self, obs_shape, feature_dim=512):\n",
    "        super().__init__()\n",
    "        # obs_shape from gym: (H, W, C) -> convert to (C, H, W) for PyTorch conv\n",
    "        if len(obs_shape) == 3:\n",
    "            # Assume gym format (H, W, C) if last dim is small (channels)\n",
    "            if obs_shape[2] <= 4:\n",
    "                self.input_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # (C, H, W)\n",
    "            else:\n",
    "                self.input_shape = obs_shape  # Already (C, H, W)\n",
    "        else:\n",
    "            self.input_shape = obs_shape\n",
    "            \n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *self.input_shape)\n",
    "            dummy = self.conv3(self.conv2(self.conv1(dummy)))\n",
    "            conv_out_size = dummy.numel()\n",
    "        \n",
    "        self.fc = nn.Linear(conv_out_size, feature_dim)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 4:  # (B, H, W, C) -> (B, C, H, W)\n",
    "            x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return F.relu(self.fc(x))\n",
    "\n",
    "class ConvGaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, hidden_dims=[256, 256], log_std_min=-20, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.cnn = NatureCNN(obs_shape, feature_dim)\n",
    "        self.mean_net = MLP(feature_dim, hidden_dims, action_dim)\n",
    "        self.log_std_net = MLP(feature_dim, hidden_dims, action_dim)\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, obs):\n",
    "        features = self.cnn(obs)\n",
    "        mean = self.mean_net(features)\n",
    "        log_std = torch.clamp(self.log_std_net(features), self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, obs, deterministic=False):\n",
    "        mean, log_std = self.forward(obs)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        if deterministic:\n",
    "            return torch.tanh(mean), None\n",
    "        \n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "class ConvCritic(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.cnn = NatureCNN(obs_shape, feature_dim)\n",
    "        self.net = MLP(feature_dim + action_dim, hidden_dims, 1)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        features = self.cnn(obs)\n",
    "        return self.net(torch.cat([features, action], dim=-1))\n",
    "\n",
    "class ConvTwinCritic(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.q1 = ConvCritic(obs_shape, action_dim, feature_dim, hidden_dims)\n",
    "        self.q2 = ConvCritic(obs_shape, action_dim, feature_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        return self.q1(obs, action), self.q2(obs, action)\n",
    "\n",
    "    def min_q(self, obs, action):\n",
    "        q1, q2 = self.forward(obs, action)\n",
    "        return torch.min(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, obs_shape, action_dim):\n",
    "        self.capacity = capacity\n",
    "        self.obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        self.obs[self.ptr] = obs\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_obs[self.ptr] = next_obs\n",
    "        self.dones[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        idxs = np.random.randint(0, self.size, batch_size)\n",
    "        batch = {\n",
    "            'obs': torch.as_tensor(self.obs[idxs], dtype=torch.float32, device=device),\n",
    "            'actions': torch.as_tensor(self.actions[idxs], dtype=torch.float32, device=device),\n",
    "            'rewards': torch.as_tensor(self.rewards[idxs], dtype=torch.float32, device=device),\n",
    "            'next_obs': torch.as_tensor(self.next_obs[idxs], dtype=torch.float32, device=device),\n",
    "            'dones': torch.as_tensor(self.dones[idxs], dtype=torch.float32, device=device)\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "class PPORolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.logps = []\n",
    "        self.dones = []\n",
    "\n",
    "    def store(self, obs, action, reward, value, logp, done):\n",
    "        self.obs.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.logps.append(logp)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def get(self):\n",
    "        data = {\n",
    "            'obs': np.array(self.obs, dtype=np.float32),\n",
    "            'actions': np.array(self.actions, dtype=np.float32),\n",
    "            'rewards': np.array(self.rewards, dtype=np.float32),\n",
    "            'values': np.array(self.values, dtype=np.float32),\n",
    "            'logps': np.array(self.logps, dtype=np.float32),\n",
    "            'dones': np.array(self.dones, dtype=np.bool_)\n",
    "        }\n",
    "        self.__init__()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb980df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper, ObservationWrapper\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class PreprocessCarRacing(ObservationWrapper):\n",
    "    def __init__(self, env, resize=(84, 84)):\n",
    "        super().__init__(env)\n",
    "        self.resize = resize\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(resize[0], resize[1], 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        cropped = gray[12:, :]  # Remove score bar\n",
    "        resized = cv2.resize(cropped, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        return resized[:, :, np.newaxis].astype(np.float32) / 255.0\n",
    "\n",
    "class FrameStack(Wrapper):\n",
    "    def __init__(self, env, num_stack=4):\n",
    "        super().__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(obs_shape[0], obs_shape[1], obs_shape[2] * num_stack),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_stack):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, term, trunc, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, term, trunc, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate(self.frames, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseAgent(ABC):\n",
    "    @abstractmethod\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train_step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.networks import GaussianPolicy, MLP\n",
    "from utils.buffers import PPORolloutBuffer\n",
    "import numpy as np\n",
    "\n",
    "class PPOAgent(BaseAgent):\n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, clip_ratio=0.2, \n",
    "                 lam=0.95, train_pi_iters=80, train_v_iters=80, target_kl=0.01,\n",
    "                 hidden_dims=[64, 64], max_ep_len=1000, ent_coef=0.0, batch_size=64, device='cuda'):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.lam = lam\n",
    "        self.train_pi_iters = train_pi_iters\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.target_kl = target_kl\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.ent_coef = ent_coef\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.actor = GaussianPolicy(obs_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic = MLP(obs_dim, hidden_dims, 1).to(device)\n",
    "        \n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.buffer = PPORolloutBuffer()\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _ = self.actor.sample(obs, deterministic)\n",
    "            value = self.critic(obs)\n",
    "        # log_prob is None when deterministic=True\n",
    "        if log_prob is None:\n",
    "            return action.cpu().numpy()[0], 0.0, value.cpu().numpy()[0].item()\n",
    "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0].item(), value.cpu().numpy()[0].item()\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        returns = np.zeros_like(rewards)\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        last_gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_non_terminal = 1.0 - dones[t]\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[t]\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * next_non_terminal - values[t]\n",
    "            advantages[t] = last_gae = delta + self.gamma * self.lam * next_non_terminal * last_gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "        \n",
    "        return returns, advantages\n",
    "\n",
    "    def train_step(self):\n",
    "        # Train when called - train.py handles the timing based on rollout_length\n",
    "        if len(self.buffer.obs) == 0:\n",
    "            return None\n",
    "            \n",
    "        data = self.buffer.get()\n",
    "        returns, advantages = self.compute_gae(data['rewards'], data['values'], data['dones'])\n",
    "        \n",
    "        old_obs = torch.as_tensor(data['obs'], dtype=torch.float32, device=self.device)\n",
    "        old_actions = torch.as_tensor(data['actions'], dtype=torch.float32, device=self.device)\n",
    "        old_logps = torch.as_tensor(data['logps'], dtype=torch.float32, device=self.device)\n",
    "        returns = torch.as_tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.as_tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        kl_divs = []\n",
    "        entropies = []\n",
    "        \n",
    "        dataset_size = len(data['obs'])\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for i in range(self.train_pi_iters):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                batch_obs = old_obs[idx]\n",
    "                batch_act = old_actions[idx]\n",
    "                batch_logp = old_logps[idx]\n",
    "                batch_adv = advantages[idx]\n",
    "                batch_ret = returns[idx]\n",
    "                \n",
    "                _, new_logps = self.actor.sample(batch_obs)\n",
    "                mean, log_std = self.actor.forward(batch_obs)\n",
    "                entropy = (0.5 + 0.5 * np.log(2 * np.pi) + log_std).sum(dim=-1)\n",
    "                ratio = torch.exp(new_logps - batch_logp.unsqueeze(1))\n",
    "                surr1 = ratio * batch_adv.unsqueeze(1)\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_adv.unsqueeze(1)\n",
    "                \n",
    "                # Entropy bonus\n",
    "                # entropy_loss = -entropy.mean()\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() #+ self.ent_coef * entropy_loss\n",
    "                \n",
    "                values = self.critic(batch_obs).squeeze()\n",
    "                critic_loss = F.mse_loss(values, batch_ret)\n",
    "                \n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=0.5)\n",
    "                self.critic_optim.step()\n",
    "                \n",
    "                kl = (batch_logp.unsqueeze(1) - new_logps).mean()\n",
    "                kl_divs.append(kl.item())\n",
    "                actor_losses.append(actor_loss.item())\n",
    "                critic_losses.append(critic_loss.item())\n",
    "                entropies.append(entropy.mean().item())\n",
    "            \n",
    "            # Early stopping based on KL divergence (averaged over epoch)\n",
    "            if np.mean(kl_divs[-dataset_size//self.batch_size:]) > self.target_kl * 1.5:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': np.mean(actor_losses),\n",
    "            'critic_loss': np.mean(critic_losses),\n",
    "            'kl_divergence': np.mean(kl_divs),\n",
    "            'training_epochs': i + 1,\n",
    "            'entropy': np.mean(entropies)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.networks import ConvGaussianPolicy, NatureCNN, MLP\n",
    "from utils.buffers import PPORolloutBuffer\n",
    "\n",
    "class PPOAgentCNN(PPOAgent):\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, **kwargs):\n",
    "        self.device = kwargs['device']\n",
    "        self.gamma = kwargs['gamma']\n",
    "        self.clip_ratio = kwargs['clip_ratio']\n",
    "        self.lam = kwargs['lam']\n",
    "        self.train_pi_iters = kwargs['train_pi_iters']\n",
    "        self.train_v_iters = kwargs['train_v_iters']\n",
    "        self.target_kl = kwargs['target_kl']\n",
    "        self.max_ep_len = kwargs['max_ep_len']\n",
    "        self.ent_coef = kwargs.get('ent_coef', 0.0)\n",
    "        self.batch_size = kwargs.get('batch_size', 64)\n",
    "        \n",
    "        # Extract only the hidden_dims for networks\n",
    "        hidden_dims = kwargs.get('hidden_dims', [256, 256])\n",
    "        \n",
    "        # CNN actor\n",
    "        self.actor = ConvGaussianPolicy(obs_shape, action_dim, feature_dim, hidden_dims=hidden_dims).to(self.device)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=kwargs['lr'])\n",
    "        \n",
    "        # CNN critic\n",
    "        class CriticCNN(torch.nn.Module):\n",
    "            def __init__(self, obs_shape, feature_dim, hidden_dims):\n",
    "                super().__init__()\n",
    "                self.cnn = NatureCNN(obs_shape, feature_dim)\n",
    "                self.fc = MLP(feature_dim, hidden_dims, 1)\n",
    "            \n",
    "            def forward(self, obs):\n",
    "                return self.fc(self.cnn(obs))\n",
    "        \n",
    "        self.critic = CriticCNN(obs_shape, feature_dim, hidden_dims).to(self.device)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=kwargs['lr'])\n",
    "        \n",
    "        self.buffer = PPORolloutBuffer()\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob = self.actor.sample(obs_tensor, deterministic)\n",
    "            value = self.critic(obs_tensor)\n",
    "        # log_prob is None when deterministic=True\n",
    "        if log_prob is None:\n",
    "            return action.cpu().numpy()[0], 0.0, value.cpu().numpy()[0].item()\n",
    "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0].item(), value.cpu().numpy()[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e05d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.networks import GaussianPolicy, TwinCritic\n",
    "from utils.buffers import ReplayBuffer\n",
    "\n",
    "class SACAgent(BaseAgent):\n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, tau=0.005, \n",
    "                 alpha=0.2, buffer_size=500000, batch_size=256, \n",
    "                 hidden_dims=[256, 256], automatic_entropy_tuning=True, \n",
    "                 target_entropy=None, device='cuda'):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.actor = GaussianPolicy(obs_dim, action_dim, hidden_dims).to(device)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic = TwinCritic(obs_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic_target = TwinCritic(obs_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        if self.automatic_entropy_tuning:\n",
    "            self.target_entropy = target_entropy if target_entropy is not None else -action_dim\n",
    "            self.log_alpha = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "            self.alpha_optim = optim.Adam([self.log_alpha], lr=lr)\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size, (obs_dim,), action_dim)\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.actor.sample(obs, deterministic)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    def train_step(self):\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return None\n",
    "            \n",
    "        batch = self.replay_buffer.sample(self.batch_size, self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob = self.actor.sample(batch['next_obs'])\n",
    "            min_q_next = self.critic_target.min_q(batch['next_obs'], next_action)\n",
    "            q_target = batch['rewards'].unsqueeze(1) + (1 - batch['dones'].unsqueeze(1)) * self.gamma * (min_q_next - self.alpha * next_log_prob)\n",
    "        \n",
    "        q1, q2 = self.critic(batch['obs'], batch['actions'])\n",
    "        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=0.5)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        action_pred, log_prob = self.actor.sample(batch['obs'])\n",
    "        q_new = self.critic.min_q(batch['obs'], action_pred)\n",
    "        actor_loss = (self.alpha * log_prob - q_new).mean()\n",
    "        \n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        alpha_loss = torch.tensor(0.0, device=self.device)\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_prob.detach() + self.target_entropy)).mean()\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'alpha_loss': alpha_loss.item(),\n",
    "            'alpha': self.alpha.item(),\n",
    "            'entropy': -log_prob.mean().item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.networks import ConvGaussianPolicy, ConvTwinCritic\n",
    "from utils.buffers import ReplayBuffer\n",
    "\n",
    "class SACAgentCNN(SACAgent):\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, **kwargs):\n",
    "        # Initialize parameters exactly like parent\n",
    "        self.device = kwargs['device']\n",
    "        self.gamma = kwargs['gamma']\n",
    "        self.tau = kwargs['tau']\n",
    "        self.alpha = kwargs.get('alpha', 0.2)\n",
    "        self.batch_size = kwargs['batch_size']\n",
    "        \n",
    "        hidden_dims = kwargs.get('hidden_dims', [256, 256])\n",
    "        \n",
    "        # --- CNN Networks ---\n",
    "        self.actor = ConvGaussianPolicy(obs_shape, action_dim, feature_dim, hidden_dims=hidden_dims).to(self.device)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=kwargs['lr'])\n",
    "        \n",
    "        self.critic = ConvTwinCritic(obs_shape, action_dim, feature_dim, hidden_dims=hidden_dims).to(self.device)\n",
    "        self.critic_target = ConvTwinCritic(obs_shape, action_dim, feature_dim, hidden_dims=hidden_dims).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=kwargs['lr'])\n",
    "        \n",
    "        self.automatic_entropy_tuning = kwargs.get('automatic_entropy_tuning', True)\n",
    "        if self.automatic_entropy_tuning:\n",
    "            self.target_entropy = kwargs.get('target_entropy', -action_dim)\n",
    "            self.log_alpha = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "            self.alpha_optim = optim.Adam([self.log_alpha], lr=kwargs['lr'])\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        # Buffer initialization\n",
    "        self.replay_buffer = ReplayBuffer(kwargs['buffer_size'], obs_shape, action_dim)\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        # Convert obs to tensor and add batch dimension\n",
    "        # Obs comes from FrameStack wrapper in (H, W, C) format, already normalized [0, 1]\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Add batch dimension if needed\n",
    "        if len(obs_tensor.shape) == 3:\n",
    "            obs_tensor = obs_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Note: No permutation here - the CNN network handles HWCâ†’CHW conversion internally\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.actor.sample(obs_tensor, deterministic)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    def train_step(self):\n",
    "        # Standard check\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return None\n",
    "            \n",
    "        batch = self.replay_buffer.sample(self.batch_size, self.device)\n",
    "        \n",
    "        # --- CNN SPECIFIC PREPROCESSING ---\n",
    "        # Observations are already normalized [0, 1] from FrameStack wrapper\n",
    "        obs = batch['obs'].float()\n",
    "        next_obs = batch['next_obs'].float()\n",
    "\n",
    "        # Note: No permutation here - the CNN network handles HWCâ†’CHW conversion internally\n",
    "\n",
    "        # Scale Rewards: CarRacing rewards are huge (~900), scale to prevent value explosion\n",
    "        rewards = batch['rewards'] / 20.0 \n",
    "        # ----------------------------------\n",
    "\n",
    "        # --- STANDARD SAC LOGIC (using processed tensors) ---\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob = self.actor.sample(next_obs)\n",
    "            # Use 'min_q' if ConvTwinCritic supports it, otherwise manually compute min(q1, q2)\n",
    "            if hasattr(self.critic_target, 'min_q'):\n",
    "                min_q_next = self.critic_target.min_q(next_obs, next_action)\n",
    "            else:\n",
    "                q1_next, q2_next = self.critic_target(next_obs, next_action)\n",
    "                min_q_next = torch.min(q1_next, q2_next)\n",
    "                \n",
    "            q_target = rewards.unsqueeze(1) + (1 - batch['dones'].unsqueeze(1)) * self.gamma * (min_q_next - self.alpha * next_log_prob)\n",
    "        \n",
    "        # Critic Update\n",
    "        q1, q2 = self.critic(obs, batch['actions'])\n",
    "        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=0.5)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # Actor Update\n",
    "        action_pred, log_prob = self.actor.sample(obs)\n",
    "        \n",
    "        if hasattr(self.critic, 'min_q'):\n",
    "            q_new = self.critic.min_q(obs, action_pred)\n",
    "        else:\n",
    "            q1_new, q2_new = self.critic(obs, action_pred)\n",
    "            q_new = torch.min(q1_new, q2_new)\n",
    "            \n",
    "        actor_loss = (self.alpha * log_prob - q_new).mean()\n",
    "        \n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        # Entropy Update\n",
    "        alpha_loss = torch.tensor(0.0, device=self.device)\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_prob.detach() + self.target_entropy)).mean()\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        # Soft Update\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'alpha_loss': alpha_loss.item(),\n",
    "            'alpha': self.alpha.item(),\n",
    "            'entropy': -log_prob.mean().item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1964a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.networks import MLP, TwinCritic\n",
    "from utils.buffers import ReplayBuffer\n",
    "import numpy as np\n",
    "\n",
    "class TD3Agent(BaseAgent):\n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, tau=0.005, \n",
    "                 policy_noise=0.2, noise_clip=0.5, policy_delay=2,\n",
    "                 buffer_size=500000, batch_size=256, hidden_dims=[256, 256], \n",
    "                 exploration_noise=0.1, device='cuda'):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.actor = MLP(obs_dim, hidden_dims, action_dim, output_activation=nn.Tanh).to(device)\n",
    "        self.actor_target = MLP(obs_dim, hidden_dims, action_dim, output_activation=nn.Tanh).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic = TwinCritic(obs_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic_target = TwinCritic(obs_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size, (obs_dim,), action_dim)\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.train_steps = 0\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(obs).cpu().numpy()[0]\n",
    "        if not deterministic:\n",
    "            action += np.random.normal(0, self.exploration_noise, size=action.shape)\n",
    "        return np.clip(action, -1.0, 1.0)\n",
    "\n",
    "    def train_step(self):\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return None\n",
    "            \n",
    "        batch = self.replay_buffer.sample(self.batch_size, self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn_like(batch['actions']) * self.policy_noise\n",
    "            noise = torch.clamp(noise, -self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_action = self.actor_target(batch['next_obs']) + noise\n",
    "            next_action = torch.clamp(next_action, -1.0, 1.0)\n",
    "            \n",
    "            q1_target, q2_target = self.critic_target(batch['next_obs'], next_action)\n",
    "            min_q_target = torch.min(q1_target, q2_target)\n",
    "            q_target = batch['rewards'].unsqueeze(1) + (1 - batch['dones'].unsqueeze(1)) * self.gamma * min_q_target\n",
    "        \n",
    "        q1, q2 = self.critic(batch['obs'], batch['actions'])\n",
    "        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=0.5)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        actor_loss = torch.tensor(0.0, device=self.device)\n",
    "        if self.train_steps % self.policy_delay == 0:\n",
    "            actor_loss = -self.critic.q1(batch['obs'], self.actor(batch['obs'])).mean()\n",
    "            \n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)\n",
    "            self.actor_optim.step()\n",
    "            \n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        self.train_steps += 1\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils.networks import NatureCNN, MLP, ConvTwinCritic\n",
    "from utils.buffers import ReplayBuffer\n",
    "\n",
    "class ConvDeterministicActor(nn.Module):\n",
    "    \"\"\"Deterministic actor for TD3 with CNN encoder\"\"\"\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        # Ensure channel dimension is passed correctly to NatureCNN\n",
    "        # obs_shape is likely (84, 84, 4) or (4, 84, 84). We handle the channel count.\n",
    "        c = obs_shape[0] if obs_shape[0] < obs_shape[2] else obs_shape[2]\n",
    "        self.cnn = NatureCNN(c, feature_dim) \n",
    "        self.net = MLP(feature_dim, hidden_dims, action_dim, output_activation=nn.Tanh)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        features = self.cnn(obs)\n",
    "        return self.net(features)\n",
    "\n",
    "class TD3AgentCNN(TD3Agent):\n",
    "    def __init__(self, obs_shape, action_dim, feature_dim=512, **kwargs):\n",
    "        self.device = kwargs['device']\n",
    "        self.gamma = kwargs['gamma']\n",
    "        self.tau = kwargs['tau']\n",
    "        self.policy_noise = kwargs['policy_noise']\n",
    "        self.noise_clip = kwargs['noise_clip']\n",
    "        self.policy_delay = kwargs['policy_delay']\n",
    "        self.batch_size = kwargs['batch_size']\n",
    "        \n",
    "        hidden_dims = kwargs.get('hidden_dims', [256, 256])\n",
    "        \n",
    "        # CNN networks\n",
    "        self.actor = ConvDeterministicActor(obs_shape, action_dim, feature_dim, hidden_dims).to(self.device)\n",
    "        self.actor_target = ConvDeterministicActor(obs_shape, action_dim, feature_dim, hidden_dims).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=kwargs['lr'])\n",
    "        \n",
    "        self.critic = ConvTwinCritic(obs_shape, action_dim, feature_dim, hidden_dims=hidden_dims).to(self.device)\n",
    "        self.critic_target = ConvTwinCritic(obs_shape, action_dim, feature_dim, hidden_dims=hidden_dims).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=kwargs['lr'])\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(kwargs['buffer_size'], obs_shape, action_dim)\n",
    "        self.exploration_noise = kwargs.get('exploration_noise', 0.1)\n",
    "        self.train_steps = 0\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        # Convert obs to tensor and add batch dimension\n",
    "        # Obs comes from FrameStack wrapper in (H, W, C) format, already normalized [0, 1]\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Add batch dimension if needed\n",
    "        if len(obs_tensor.shape) == 3:\n",
    "            obs_tensor = obs_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Note: No permutation here - the CNN network handles HWCâ†’CHW conversion internally\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(obs_tensor).cpu().numpy()[0]\n",
    "            \n",
    "        if not deterministic:\n",
    "            # Add noise for exploration\n",
    "            noise = np.random.normal(0, self.exploration_noise, size=action.shape)\n",
    "            action = action + noise\n",
    "            \n",
    "        return np.clip(action, -1.0, 1.0)\n",
    "\n",
    "    def train_step(self):\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        self.train_steps += 1\n",
    "        batch = self.replay_buffer.sample(self.batch_size, self.device)\n",
    "        \n",
    "        # --- CNN SPECIFIC PREPROCESSING ---\n",
    "        # Observations are already normalized [0, 1] from FrameStack wrapper\n",
    "        obs = batch['obs'].float()\n",
    "        next_obs = batch['next_obs'].float()\n",
    "        \n",
    "        # Note: No permutation here - the CNN network handles HWCâ†’CHW conversion internally\n",
    "        \n",
    "        # Scale Rewards: CarRacing rewards are huge (~900), scale to prevent value explosion\n",
    "        rewards = batch['rewards'] / 20.0\n",
    "        # ----------------------------------\n",
    "\n",
    "        # Critic Update\n",
    "        with torch.no_grad():\n",
    "            # Select action according to target actor and add clipped noise\n",
    "            noise = (torch.randn_like(batch['actions']) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_actions = (self.actor_target(next_obs) + noise).clamp(-1, 1)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_q1, target_q2 = self.critic_target(next_obs, next_actions)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "            target_q = rewards.unsqueeze(1) + (1 - batch['dones'].unsqueeze(1)) * self.gamma * target_q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_q1, current_q2 = self.critic(obs, batch['actions'])\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        actor_loss = None\n",
    "        \n",
    "        # Delayed policy updates\n",
    "        if self.train_steps % self.policy_delay == 0:\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.q1(obs, self.actor(obs)).mean()\n",
    "            \n",
    "            # Optimize the actor\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "            self.actor_optim.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        # Return metrics (check if actor_loss was computed)\n",
    "        metrics = {'critic_loss': critic_loss.item()}\n",
    "        if actor_loss is not None:\n",
    "            metrics['actor_loss'] = actor_loss.item()\n",
    "            \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8933b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e45208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc50ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b77a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53ad99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133a14c1",
   "metadata": {},
   "source": [
    "# ðŸŽ® RL Assignment 4 - Complete Training Pipeline\n",
    "## Kaggle-Ready Implementation\n",
    "\n",
    "This notebook contains a complete implementation of:\n",
    "- **SAC** (Soft Actor-Critic) - MLP & CNN variants\n",
    "- **TD3** (Twin Delayed DDPG) - MLP & CNN variants  \n",
    "- **PPO** (Proximal Policy Optimization) - MLP & CNN variants\n",
    "\n",
    "**Environments:**\n",
    "- LunarLander-v3 (MLP agents)\n",
    "- CarRacing-v3 (CNN agents)\n",
    "\n",
    "**Features:**\n",
    "- All code self-contained (no external files needed)\n",
    "- Easy configuration switching\n",
    "- WandB integration for tracking\n",
    "- Video recording of evaluations\n",
    "- Checkpoint saving/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c8b6c",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium[box2d] -q\n",
    "!pip install wandb -q\n",
    "!pip install opencv-python -q\n",
    "!pip install swig -q\n",
    "\n",
    "print(\"âœ“ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('videos', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ðŸ–¥ï¸  Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70538eb0",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration System\n",
    "\n",
    "Select which experiment to run by changing the `EXPERIMENT` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2de8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ðŸŽ¯ SELECT EXPERIMENT HERE\n",
    "# ========================================\n",
    "EXPERIMENT = \"sac_carracing\"  # Options: sac_carracing, td3_carracing, ppo_carracing, \n",
    "                              #          sac_lunarlander, td3_lunarlander, ppo_lunarlander\n",
    "\n",
    "# WandB Configuration (Set to None to disable)\n",
    "WANDB_ENTITY = \"ziadhf-cairo-university\"  # Your WandB username/entity\n",
    "WANDB_PROJECT = \"cmps458-assignment4_2\"   # Your project name\n",
    "WANDB_ENABLED = True                      # Set to False to disable WandB\n",
    "\n",
    "# Training Configuration\n",
    "CHECKPOINT_FREQ = 100000  # Save checkpoint every N steps\n",
    "ENABLE_VIDEO = True       # Record evaluation videos\n",
    "\n",
    "print(f\"ðŸŽ¯ Selected Experiment: {EXPERIMENT}\")\n",
    "print(f\"ðŸ“Š WandB: {'Enabled' if WANDB_ENABLED else 'Disabled'}\")\n",
    "print(f\"ðŸŽ¥ Video Recording: {'Enabled' if ENABLE_VIDEO else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082bb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Authentication (for Kaggle)\n",
    "# Add your WandB API key as a Kaggle Secret named 'WANDB_API_KEY'\n",
    "# Or uncomment and paste your API key directly (not recommended for public notebooks)\n",
    "\n",
    "if WANDB_ENABLED:\n",
    "    try:\n",
    "        # Try to get from Kaggle secrets\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "        os.environ['WANDB_API_KEY'] = wandb_key\n",
    "        print(\"âœ“ WandB API key loaded from Kaggle secrets\")\n",
    "    except:\n",
    "        # If not on Kaggle or secret not set, try environment variable\n",
    "        if 'WANDB_API_KEY' not in os.environ:\n",
    "            print(\"âš ï¸  WandB API key not found!\")\n",
    "            print(\"   Option 1: Add 'WANDB_API_KEY' to Kaggle Secrets\")\n",
    "            print(\"   Option 2: Set WANDB_ENABLED = False to disable logging\")\n",
    "            # Uncomment and paste your key here (not recommended for public notebooks):\n",
    "            # os.environ['WANDB_API_KEY'] = 'your-api-key-here'\n",
    "        else:\n",
    "            print(\"âœ“ WandB API key found in environment\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  WandB logging disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Dictionary\n",
    "CONFIGS = {\n",
    "    \"sac_carracing\": {\n",
    "        'algo': 'sac_cnn',\n",
    "        'env_id': 'CarRacing-v3',\n",
    "        'use_cnn': True,\n",
    "        'feature_dim': 512,\n",
    "        'seed': 42,\n",
    "        'device': device,\n",
    "        'total_steps': 2000000,\n",
    "        'learning_starts': 35000,\n",
    "        'train_freq': 1,\n",
    "        'gradient_steps': 1,\n",
    "        'max_ep_len': 1000,\n",
    "        'eval_interval': 25000,\n",
    "        'eval_episodes': 5,\n",
    "        'checkpoint_freq': CHECKPOINT_FREQ,\n",
    "        'agent_params': {\n",
    "            'lr': 0.0001,\n",
    "            'gamma': 0.99,\n",
    "            'tau': 0.005,\n",
    "            'alpha': 0.2,\n",
    "            'automatic_entropy_tuning': True,\n",
    "            'buffer_size': 200000,\n",
    "            'batch_size': 256,\n",
    "            'hidden_dims': [256, 256]\n",
    "        }\n",
    "    },\n",
    "    \"td3_carracing\": {\n",
    "        'algo': 'td3_cnn',\n",
    "        'env_id': 'CarRacing-v3',\n",
    "        'use_cnn': True,\n",
    "        'feature_dim': 512,\n",
    "        'seed': 42,\n",
    "        'device': device,\n",
    "        'total_steps': 2000000,\n",
    "        'learning_starts': 35000,\n",
    "        'train_freq': 1,\n",
    "        'gradient_steps': 1,\n",
    "        'max_ep_len': 1000,\n",
    "        'eval_interval': 25000,\n",
    "        'eval_episodes': 3,\n",
    "        'checkpoint_freq': CHECKPOINT_FREQ,\n",
    "        'agent_params': {\n",
    "            'lr': 0.0001,\n",
    "            'gamma': 0.99,\n",
    "            'tau': 0.005,\n",
    "            'policy_noise': 0.2,\n",
    "            'noise_clip': 0.5,\n",
    "            'policy_delay': 2,\n",
    "            'buffer_size': 200000,\n",
    "            'batch_size': 256,\n",
    "            'hidden_dims': [256, 256],\n",
    "            'exploration_noise': 0.3\n",
    "        }\n",
    "    },\n",
    "    \"ppo_carracing\": {\n",
    "        'algo': 'ppo_cnn',\n",
    "        'env_id': 'CarRacing-v3',\n",
    "        'use_cnn': True,\n",
    "        'feature_dim': 512,\n",
    "        'seed': 42,\n",
    "        'device': device,\n",
    "        'total_steps': 3000000,\n",
    "        'learning_starts': 0,\n",
    "        'rollout_length': 4096,\n",
    "        'train_freq': 4096,\n",
    "        'gradient_steps': 1,\n",
    "        'max_ep_len': 2000,\n",
    "        'eval_interval': 50000,\n",
    "        'eval_episodes': 3,\n",
    "        'checkpoint_freq': CHECKPOINT_FREQ,\n",
    "        'agent_params': {\n",
    "            'lr': 0.0003,\n",
    "            'gamma': 0.99,\n",
    "            'clip_ratio': 0.2,\n",
    "            'lam': 0.95,\n",
    "            'train_pi_iters': 15,\n",
    "            'train_v_iters': 15,\n",
    "            'target_kl': 0.02,\n",
    "            'ent_coef': 0.01,\n",
    "            'batch_size': 128,\n",
    "            'hidden_dims': [256, 256],\n",
    "            'max_ep_len': 1000\n",
    "        }\n",
    "    },\n",
    "    \"sac_lunarlander\": {\n",
    "        'algo': 'sac',\n",
    "        'env_id': 'LunarLander-v3',\n",
    "        'use_cnn': False,\n",
    "        'seed': 42,\n",
    "        'device': device,\n",
    "        'total_steps': 300000,\n",
    "        'learning_starts': 10000,\n",
    "        'train_freq': 1,\n",
    "        'gradient_steps': 1,\n",
    "        'max_ep_len': 1000,\n",
    "        'eval_interval': 10000,\n",
    "        'eval_episodes': 10,\n",
    "        'checkpoint_freq': CHECKPOINT_FREQ,\n",
    "        'agent_params': {\n",
    "            'lr': 0.0003,\n",
    "            'gamma': 0.99,\n",
    "            'tau': 0.005,\n",
    "            'alpha': 0.2,\n",
    "            'automatic_entropy_tuning': True,\n",
    "            'buffer_size': 1000000,\n",
    "            'batch_size': 256,\n",
    "            'hidden_dims': [256, 256]\n",
    "        }\n",
    "    },\n",
    "    \"td3_lunarlander\": {\n",
    "        'algo': 'td3',\n",
    "        'env_id': 'LunarLander-v3',\n",
    "        'use_cnn': False,\n",
    "        'seed': 42,\n",
    "        'device': device,\n",
    "        'total_steps': 300000,\n",
    "        'learning_starts': 10000,\n",
    "        'train_freq': 1,\n",
    "        'gradient_steps': 1,\n",
    "        'max_ep_len': 1000,\n",
    "        'eval_interval': 10000,\n",
    "        'eval_episodes': 10,\n",
    "        'checkpoint_freq': CHECKPOINT_FREQ,\n",
    "        'agent_params': {\n",
    "            'lr': 0.0003,\n",
    "            'gamma': 0.99,\n",
    "            'tau': 0.005,\n",
    "            'policy_noise': 0.2,\n",
    "            'noise_clip': 0.5,\n",
    "            'policy_delay': 2,\n",
    "            'buffer_size': 1000000,\n",
    "            'batch_size': 256,\n",
    "            'hidden_dims': [256, 256],\n",
    "            'exploration_noise': 0.1\n",
    "        }\n",
    "    },\n",
    "    \"ppo_lunarlander\": {\n",
    "        'algo': 'ppo',\n",
    "        'env_id': 'LunarLander-v3',\n",
    "        'use_cnn': False,\n",
    "        'seed': 42,\n",
    "        'device': device,\n",
    "        'total_steps': 1000000,\n",
    "        'learning_starts': 0,\n",
    "        'rollout_length': 2048,\n",
    "        'train_freq': 2048,\n",
    "        'gradient_steps': 1,\n",
    "        'max_ep_len': 1000,\n",
    "        'eval_interval': 20000,\n",
    "        'eval_episodes': 10,\n",
    "        'checkpoint_freq': CHECKPOINT_FREQ,\n",
    "        'agent_params': {\n",
    "            'lr': 0.0003,\n",
    "            'gamma': 0.99,\n",
    "            'clip_ratio': 0.2,\n",
    "            'lam': 0.95,\n",
    "            'train_pi_iters': 80,\n",
    "            'train_v_iters': 80,\n",
    "            'target_kl': 0.01,\n",
    "            'ent_coef': 0.0,\n",
    "            'batch_size': 64,\n",
    "            'hidden_dims': [64, 64],\n",
    "            'max_ep_len': 1000\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load selected config\n",
    "config = CONFIGS[EXPERIMENT]\n",
    "config['run_name'] = f\"{config['algo']}-{config['env_id']}-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"\\nðŸ“‹ Configuration Loaded:\")\n",
    "print(f\"   Algorithm: {config['algo']}\")\n",
    "print(f\"   Environment: {config['env_id']}\")\n",
    "print(f\"   Total Steps: {config['total_steps']:,}\")\n",
    "print(f\"   Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb52ce",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93366158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "def make_env(env_id, seed=42, use_cnn=False, capture_video=False, run_name=None):\n",
    "    \"\"\"Create environment with optional CNN preprocessing and video recording\"\"\"\n",
    "    render_mode = 'rgb_array' if capture_video else None\n",
    "    env = gym.make(env_id, continuous=True, render_mode=render_mode)\n",
    "    \n",
    "    if capture_video and run_name and ENABLE_VIDEO:\n",
    "        video_folder = f\"videos/{run_name}\"\n",
    "        env = RecordVideo(\n",
    "            env, \n",
    "            video_folder=video_folder,\n",
    "            episode_trigger=lambda x: True,\n",
    "            disable_logger=True\n",
    "        )\n",
    "    \n",
    "    if use_cnn:\n",
    "        env = PreprocessCarRacing(env, resize=(84, 84))\n",
    "        env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "def evaluate(agent, env, n_episodes=5, max_ep_len=1000):\n",
    "    \"\"\"Evaluate agent across multiple episodes\"\"\"\n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < max_ep_len:\n",
    "            if isinstance(agent, (PPOAgent, PPOAgentCNN)):\n",
    "                action, _, _ = agent.select_action(obs, deterministic=True)\n",
    "            else:\n",
    "                action = agent.select_action(obs, deterministic=True)\n",
    "            \n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            ep_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(rewards),\n",
    "        'std': np.std(rewards),\n",
    "        'min': np.min(rewards),\n",
    "        'max': np.max(rewards)\n",
    "    }\n",
    "\n",
    "def save_checkpoint(agent, path, config, eval_score, step):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    save_dict = {\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'config': config,\n",
    "        'eval_score': eval_score,\n",
    "        'step': step\n",
    "    }\n",
    "    if hasattr(agent, 'critic'):\n",
    "        save_dict['critic_state_dict'] = agent.critic.state_dict()\n",
    "    \n",
    "    torch.save(save_dict, path)\n",
    "    print(f\"ðŸ’¾ Checkpoint saved: {path}\")\n",
    "\n",
    "def load_checkpoint(agent, checkpoint_path):\n",
    "    \"\"\"Load checkpoint and return step number and best eval score\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=agent.device, weights_only=False)\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    if hasattr(agent, 'critic') and 'critic_state_dict' in checkpoint:\n",
    "        agent.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "    print(f\"âœ… Loaded checkpoint from step {checkpoint['step']}, score: {checkpoint['eval_score']:.2f}\")\n",
    "    return checkpoint['step'], checkpoint['eval_score']\n",
    "\n",
    "print(\"âœ“ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3db7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(config, env):\n",
    "    \"\"\"Factory to create the correct agent based on config\"\"\"\n",
    "    algo = config['algo']\n",
    "    use_cnn = config.get('use_cnn', False)\n",
    "    \n",
    "    # Parameters to filter based on algorithm type\n",
    "    ppo_only = ['max_ep_len', 'lam', 'clip_ratio', 'train_pi_iters', 'train_v_iters', 'target_kl']\n",
    "    sac_td3_only = ['tau', 'alpha', 'automatic_entropy_tuning', 'buffer_size', \n",
    "                    'policy_noise', 'noise_clip', 'policy_delay', 'exploration_noise']\n",
    "    \n",
    "    is_ppo = 'ppo' in algo.lower()\n",
    "    filter_params = sac_td3_only + ['obs_shape', 'feature_dim'] if is_ppo else ppo_only + ['obs_shape', 'feature_dim']\n",
    "    \n",
    "    base_params = {\n",
    "        'action_dim': env.action_space.shape[0],\n",
    "        'device': config['device'],\n",
    "        **{k: v for k, v in config['agent_params'].items() if k not in filter_params}\n",
    "    }\n",
    "    \n",
    "    if use_cnn:\n",
    "        base_params['obs_shape'] = env.observation_space.shape\n",
    "        base_params['feature_dim'] = config.get('feature_dim', 512)\n",
    "        \n",
    "        agent_map = {\n",
    "            'sac': SACAgentCNN, 'td3': TD3AgentCNN, 'ppo': PPOAgentCNN,\n",
    "            'sac_cnn': SACAgentCNN, 'td3_cnn': TD3AgentCNN, 'ppo_cnn': PPOAgentCNN\n",
    "        }\n",
    "    else:\n",
    "        base_params['obs_dim'] = env.observation_space.shape[0]\n",
    "        agent_map = {'sac': SACAgent, 'td3': TD3Agent, 'ppo': PPOAgent}\n",
    "    \n",
    "    agent_class = agent_map.get(algo.lower())\n",
    "    if agent_class is None:\n",
    "        raise ValueError(f\"Unknown algorithm: {algo}\")\n",
    "    \n",
    "    return agent_class(**base_params)\n",
    "\n",
    "print(\"âœ“ Agent factory defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ecea5",
   "metadata": {},
   "source": [
    "## ðŸš€ Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    run_name = config['run_name']\n",
    "    \n",
    "    # Initialize WandB\n",
    "    if WANDB_ENABLED and WANDB_ENTITY:\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_ENTITY,\n",
    "            config=config,\n",
    "            name=run_name,\n",
    "            tags=[config['algo'], config['env_id']],\n",
    "            monitor_gym=True,\n",
    "            save_code=True\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ¯ Starting Training: {run_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create environments\n",
    "    use_cnn = config.get('use_cnn', False)\n",
    "    env = make_env(config['env_id'], seed=config['seed'], use_cnn=use_cnn, capture_video=False)\n",
    "    eval_env = make_env(config['env_id'], seed=config['seed']+100, use_cnn=use_cnn, \n",
    "                        capture_video=True, run_name=run_name)\n",
    "    \n",
    "    print(f\"ðŸŒ Environment: {config['env_id']}\")\n",
    "    print(f\"   Observation space: {env.observation_space}\")\n",
    "    print(f\"   Action space: {env.action_space}\")\n",
    "    print(f\"   CNN Mode: {'ON' if use_cnn else 'OFF'}\\n\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = create_agent(config, env)\n",
    "    print(f\"ðŸ¤– Agent: {agent.__class__.__name__}\\n\")\n",
    "    \n",
    "    # Training variables\n",
    "    obs, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    best_eval_score = -np.inf\n",
    "    episode_count = 0\n",
    "    episode_rewards_history = []\n",
    "    last_log_step = 0\n",
    "    log_freq = 1000\n",
    "    \n",
    "    # Prefill replay buffer for off-policy agents\n",
    "    prefill_steps = config.get('learning_starts', 0)\n",
    "    if prefill_steps > 0 and not isinstance(agent, (PPOAgent, PPOAgentCNN)):\n",
    "        print(f\"ðŸ”„ Prefilling replay buffer ({prefill_steps} steps)...\")\n",
    "        while len(agent.replay_buffer) < prefill_steps:\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            agent.replay_buffer.add(obs, action, reward, next_obs, term or trunc)\n",
    "            obs = next_obs\n",
    "            if term or trunc:\n",
    "                obs, _ = env.reset()\n",
    "        print(\"âœ… Buffer prefilled\\n\")\n",
    "        obs, _ = env.reset()\n",
    "    \n",
    "    # Main training loop\n",
    "    print(\"ðŸƒ Training started...\\n\")\n",
    "    for step in range(config['total_steps']):\n",
    "        # Select action\n",
    "        if isinstance(agent, (PPOAgent, PPOAgentCNN)):\n",
    "            action, logp, val = agent.select_action(obs, deterministic=False)\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            agent.buffer.store(obs, action, reward, val, logp, done)\n",
    "        else:\n",
    "            action = agent.select_action(obs, deterministic=False)\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            agent.replay_buffer.add(obs, action, reward, next_obs, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        # Training\n",
    "        metrics = None\n",
    "        should_train = False\n",
    "        \n",
    "        if isinstance(agent, (PPOAgent, PPOAgentCNN)):\n",
    "            should_train = len(agent.buffer.obs) >= config.get('rollout_length', 2048)\n",
    "        else:\n",
    "            should_train = step > prefill_steps and step % config['train_freq'] == 0\n",
    "        \n",
    "        if should_train:\n",
    "            if isinstance(agent, (PPOAgent, PPOAgentCNN)):\n",
    "                metrics = agent.train_step()\n",
    "            else:\n",
    "                for _ in range(config['gradient_steps']):\n",
    "                    metrics = agent.train_step()\n",
    "            \n",
    "            if metrics and WANDB_ENABLED:\n",
    "                wandb.log({f'train/{k}': v for k, v in metrics.items()}, step=step)\n",
    "        \n",
    "        # Checkpoint saving\n",
    "        if step > 0 and step % config['checkpoint_freq'] == 0:\n",
    "            ckpt_path = f\"models/{run_name}/checkpoint_{step}.pth\"\n",
    "            save_checkpoint(agent, ckpt_path, config, best_eval_score, step)\n",
    "        \n",
    "        # Episode end\n",
    "        if done or episode_length >= config.get('max_ep_len', 1000):\n",
    "            episode_count += 1\n",
    "            episode_rewards_history.append(episode_reward)\n",
    "            if len(episode_rewards_history) > 100:\n",
    "                episode_rewards_history.pop(0)\n",
    "            \n",
    "            success = False\n",
    "            if 'CarRacing' in config['env_id']:\n",
    "                success = episode_reward >= 900\n",
    "            \n",
    "            if WANDB_ENABLED:\n",
    "                wandb.log({\n",
    "                    'train/episode_reward': episode_reward,\n",
    "                    'train/episode_length': episode_length,\n",
    "                    'train/episode_count': episode_count,\n",
    "                    'train/success': int(success),\n",
    "                    'train/rolling_mean_reward': np.mean(episode_rewards_history),\n",
    "                }, step=step)\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            # Console progress\n",
    "            if step - last_log_step >= log_freq:\n",
    "                progress = 100 * step / config['total_steps']\n",
    "                recent_mean = np.mean(episode_rewards_history[-10:]) if episode_rewards_history else 0\n",
    "                print(f\"ðŸ“Š Step {step:,}/{config['total_steps']:,} ({progress:.1f}%) | \"\n",
    "                      f\"Ep: {episode_count} | R_avg: {recent_mean:.1f} | Best: {best_eval_score:.1f}\")\n",
    "                last_log_step = step\n",
    "        \n",
    "        # Evaluation\n",
    "        if step % config['eval_interval'] == 0 and step > 0:\n",
    "            print(f\"\\nðŸŽ¯ Evaluating at step {step:,}...\")\n",
    "            eval_results = evaluate(agent, eval_env, n_episodes=config['eval_episodes'])\n",
    "            \n",
    "            if WANDB_ENABLED:\n",
    "                wandb.log({f'eval/{k}': v for k, v in eval_results.items()}, step=step)\n",
    "            \n",
    "            print(f\"   Mean: {eval_results['mean']:.2f} Â± {eval_results['std']:.2f}\")\n",
    "            print(f\"   Range: [{eval_results['min']:.2f}, {eval_results['max']:.2f}]\")\n",
    "            \n",
    "            if eval_results['mean'] > best_eval_score:\n",
    "                best_eval_score = eval_results['mean']\n",
    "                print(f\"   ðŸŒŸ New best score!\")\n",
    "                best_path = f\"models/{run_name}/best_model.pth\"\n",
    "                save_checkpoint(agent, best_path, config, best_eval_score, step)\n",
    "            print()\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "    \n",
    "    # Final save\n",
    "    final_path = f\"models/{run_name}/final_model.pth\"\n",
    "    save_checkpoint(agent, final_path, config, best_eval_score, config['total_steps'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… Training Complete!\")\n",
    "    print(f\"   Best Score: {best_eval_score:.2f}\")\n",
    "    print(f\"   Models saved in: models/{run_name}/\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    if WANDB_ENABLED:\n",
    "        wandb.finish()\n",
    "    \n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    return best_eval_score\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f62770",
   "metadata": {},
   "source": [
    "## â–¶ï¸ RUN TRAINING\n",
    "\n",
    "Execute the cell below to start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ START TRAINING\n",
    "try:\n",
    "    best_score = train(config)\n",
    "    print(f\"\\nðŸŽ‰ Training finished successfully!\")\n",
    "    print(f\"ðŸ† Best evaluation score: {best_score:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29f1a1",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluation & Visualization\n",
    "\n",
    "Use these cells to load a trained model and evaluate it, or to view recorded videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabcc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate a trained model\n",
    "def evaluate_model(model_path, n_episodes=10):\n",
    "    \"\"\"Load a checkpoint and evaluate it\"\"\"\n",
    "    # Load config from checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    eval_config = checkpoint['config']\n",
    "    \n",
    "    # Create environment and agent\n",
    "    use_cnn = eval_config.get('use_cnn', False)\n",
    "    env = make_env(eval_config['env_id'], seed=42, use_cnn=use_cnn, capture_video=False)\n",
    "    agent = create_agent(eval_config, env)\n",
    "    \n",
    "    # Load weights\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    if hasattr(agent, 'critic') and 'critic_state_dict' in checkpoint:\n",
    "        agent.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "    \n",
    "    print(f\"ðŸ“‚ Loaded model from: {model_path}\")\n",
    "    print(f\"   Step: {checkpoint['step']}\")\n",
    "    print(f\"   Saved eval score: {checkpoint['eval_score']:.2f}\\n\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"ðŸŽ¯ Evaluating for {n_episodes} episodes...\")\n",
    "    results = evaluate(agent, env, n_episodes=n_episodes)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"   Mean: {results['mean']:.2f} Â± {results['std']:.2f}\")\n",
    "    print(f\"   Range: [{results['min']:.2f}, {results['max']:.2f}]\")\n",
    "    \n",
    "    env.close()\n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# model_path = \"models/sac_cnn-CarRacing-v3-20251210_035927/best_model.pth\"\n",
    "# evaluate_model(model_path, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved models\n",
    "import glob\n",
    "\n",
    "print(\"ðŸ’¾ Available Models:\\n\")\n",
    "models = glob.glob(\"models/*/best_model.pth\")\n",
    "if models:\n",
    "    for i, model in enumerate(models, 1):\n",
    "        try:\n",
    "            checkpoint = torch.load(model, map_location='cpu', weights_only=False)\n",
    "            print(f\"{i}. {model}\")\n",
    "            print(f\"   Step: {checkpoint.get('step', 'N/A')}, Score: {checkpoint.get('eval_score', 'N/A'):.2f}\\n\")\n",
    "        except:\n",
    "            print(f\"{i}. {model} (could not load info)\\n\")\n",
    "else:\n",
    "    print(\"No models found. Train a model first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recorded videos (for Jupyter/Kaggle)\n",
    "from IPython.display import Video, display\n",
    "import glob\n",
    "\n",
    "def show_videos(run_name=None, max_videos=3):\n",
    "    \"\"\"Display evaluation videos\"\"\"\n",
    "    if run_name:\n",
    "        video_pattern = f\"videos/{run_name}/*.mp4\"\n",
    "    else:\n",
    "        video_pattern = \"videos/**/*.mp4\"\n",
    "    \n",
    "    videos = glob.glob(video_pattern, recursive=True)\n",
    "    \n",
    "    if not videos:\n",
    "        print(\"No videos found. Enable video recording and run evaluation.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸŽ¥ Found {len(videos)} videos\\n\")\n",
    "    \n",
    "    for i, video_path in enumerate(videos[:max_videos], 1):\n",
    "        print(f\"Video {i}: {video_path}\")\n",
    "        try:\n",
    "            display(Video(video_path, width=600))\n",
    "        except:\n",
    "            print(f\"Could not display {video_path}\\n\")\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# show_videos(run_name=\"sac_cnn-CarRacing-v3-20251210_035927\", max_videos=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f190b9",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Usage Instructions\n",
    "\n",
    "### To run on Kaggle:\n",
    "\n",
    "1. **Upload this notebook** to Kaggle\n",
    "2. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2\n",
    "3. **Set experiment**: Change `EXPERIMENT` variable in the configuration cell\n",
    "4. **Configure WandB** (optional): \n",
    "   - Set `WANDB_ENTITY` to your username\n",
    "   - Set `WANDB_ENABLED = True`\n",
    "   - Add WandB API key in Kaggle Secrets as `WANDB_API_KEY`\n",
    "5. **Run all cells** from top to bottom\n",
    "\n",
    "### Available Experiments:\n",
    "- `sac_carracing` - SAC on CarRacing (2M steps, ~6-8 hours)\n",
    "- `td3_carracing` - TD3 on CarRacing (2M steps, ~6-8 hours)\n",
    "- `ppo_carracing` - PPO on CarRacing (3M steps, ~8-10 hours)\n",
    "- `sac_lunarlander` - SAC on LunarLander (300K steps, ~1 hour)\n",
    "- `td3_lunarlander` - TD3 on LunarLander (300K steps, ~1 hour)\n",
    "- `ppo_lunarlander` - PPO on LunarLander (1M steps, ~2-3 hours)\n",
    "\n",
    "### Tips:\n",
    "- Use **GPU** for faster training (5-10x speedup)\n",
    "- Enable **Internet** in Kaggle settings for WandB logging\n",
    "- Models are saved in `models/` directory\n",
    "- Videos are saved in `videos/` directory\n",
    "- Checkpoints are saved every 100K steps by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
